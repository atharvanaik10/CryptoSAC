{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soft_actor_critic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlVI6Dx9PVimNXrRokrReM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharvanaik10/CryptoSAC/blob/main/soft_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discrete Soft Actor Critic (SAC) for crypto trading"
      ],
      "metadata": {
        "id": "n3HQ3lKIaX6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-R3fBaD-g14"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Memory Buffer"
      ],
      "metadata": {
        "id": "-yxM40FiGgNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o5xtH8_2kFpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "  def push(self, *args):\n",
        "    self.memory.append(Transition(*args))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "2xzyGL3AGShv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critic Network"
      ],
      "metadata": {
        "id": "6u6sIKTqKbZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams for critic layers\n",
        "LAYER_1_SIZE = 128\n",
        "LAYER_2_SIZE = 256\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dims, learning_rate):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.state_dims = state_dims\n",
        "    self.action_dims = action_dims\n",
        "\n",
        "    self.layer1 = nn.Linear(self.state_dims + action_dims, LAYER_1_SIZE)\n",
        "    self.layer2 = nn.Linear(LAYER_1_SIZE, LAYER_2_SIZE)\n",
        "    self.outlayer = nn.Linear(LAYER_2_SIZE, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    # Calculate the quality of the state and action Q(s,a)\n",
        "    quality = self.layer1(T.cat([state, action], dim=1))\n",
        "    quality = F.relu(quality)\n",
        "\n",
        "    quality = self.layer2(quality)\n",
        "    quality = F.relu(quality)\n",
        "\n",
        "    quality = self.outlayer(quality)\n",
        "\n",
        "    return quality"
      ],
      "metadata": {
        "id": "vWMCqRZOKdCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor Network"
      ],
      "metadata": {
        "id": "RWNHPJFvOOpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams for actor layers\n",
        "LAYER_1_SIZE = 128\n",
        "LAYER_2_SIZE = 256\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dims, action_dims, learning_rate):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.state_dims = state_dims\n",
        "        self.action_dims = action_dims\n",
        "\n",
        "        self.layer1 = nn.Linear(self.state_dims, LAYER_1_SIZE)\n",
        "        self.layer2 = nn.Linear(LAYER_1_SIZE, LAYER_2_SIZE)\n",
        "        self.outlayer = nn.Linear(LAYER_2_SIZE, action_dims)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Calculate the policy probablity pi_a(s)\n",
        "        policy = self.layer1(state)\n",
        "        policy = F.relu(policy)\n",
        "\n",
        "        policy = self.layer2(policy)\n",
        "        policy = F.relu(policy)\n",
        "\n",
        "        policy = self.outlayer(policy)\n",
        "        policy = F.softmax(policy, dim=-1)\n",
        "\n",
        "        return policy\n"
      ],
      "metadata": {
        "id": "xWZ3I--sOQR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}