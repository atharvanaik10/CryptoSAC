{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soft_actor_critic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgP0EVj74yyPYPS1tPcBZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharvanaik10/CryptoSAC/blob/main/soft_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soft Actor Critic RL for discrete action spaces and continous state spaces"
      ],
      "metadata": {
        "id": "n3HQ3lKIaX6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-R3fBaD-g14"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.activation import ReLU\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experience Replay Memory Buffer"
      ],
      "metadata": {
        "id": "-yxM40FiGgNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', \n",
        "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "  def push(self, *args):\n",
        "    self.memory.append(Transition(*args))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "2xzyGL3AGShv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critic Network\n",
        "With action space $A$, the critic is a function approximator defined by Q space\n",
        "$$ Q : \\mathcal{S} \\rightarrow \\mathbb{R}^{|A|} $$"
      ],
      "metadata": {
        "id": "6u6sIKTqKbZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dims, learning_rate, \n",
        "               layer1_size, layer2_size):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.state_dims = state_dims\n",
        "    self.action_dims = action_dims\n",
        "\n",
        "    self.model = nn.Sequential(nn.Linear(state_dims, layer1_size),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Linear(layer1_size, layer2_size),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Linear(layer2_size, 1))\n",
        "\n",
        "    # self.layer1 = nn.Linear(self.state_dims + action_dims, layer1_size)\n",
        "    # self.layer2 = nn.Linear(layer1_size, layer2_size)\n",
        "    # self.outlayer = nn.Linear(layer2_size, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    self.to(device)\n",
        "    \n",
        "  def forward(self, state, action):\n",
        "    # Calculate the quality of the state and action Q(s,a)\n",
        "    quality = self.model(state)\n",
        "    return quality"
      ],
      "metadata": {
        "id": "vWMCqRZOKdCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor Network\n",
        "With action space $A$, the actor is a function approximator defined by pi space\n",
        "$$ \\pi : \\mathcal{S} \\rightarrow [0,1]^{|A|} $$"
      ],
      "metadata": {
        "id": "RWNHPJFvOOpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dims, action_dims, learning_rate, \n",
        "                 layer1_size, layer2_size):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.state_dims = state_dims\n",
        "        self.action_dims = action_dims\n",
        "    \n",
        "        self.model = nn.Sequential(nn.Linear(state_dims, layer1_size),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(layer1_size, layer2_size),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(layer2_size, action_dims),\n",
        "                                   nn.Softmax(dim=-1))\n",
        "\n",
        "        # self.layer1 = nn.Linear(self.state_dims, layer1_size)\n",
        "        # self.layer2 = nn.Linear(layer1_size, layer2_size)\n",
        "        # self.outlayer = nn.Linear(layer2_size, action_dims)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Calculate the policy probablity pi_a(s)\n",
        "        policy = self.model(state)\n",
        "        return policy"
      ],
      "metadata": {
        "id": "xWZ3I--sOQR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent\n"
      ],
      "metadata": {
        "id": "lC_IzHK3cLbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    # alpha = entropy temperature factor\n",
        "    # gamma = discount factor\n",
        "    # tau = soft update interpolation factor\n",
        "    def __init__(self, env, alpha=1, gamma=0.99, learning_rate=10e-4, \n",
        "                 tau=0.01, buffer_capacity=1000000, batch_size=100):\n",
        "        self.env = env\n",
        "        self.state_dims = 5\n",
        "        self.action_dims = 3\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.learning_rate = learning rate\n",
        "\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # initialize replay buffer\n",
        "        self.replay_buffer = ReplayMemory(self.buffer_capacity)\n",
        "\n",
        "        # initialise 2x critics (Q-local) and 2x target critics (Q)\n",
        "        self.local_critic1 = Critic(self.state_dims, self.action_dims, \n",
        "                                    self.learning_rate, 256, 256)\n",
        "        self.local_critic2 = Critic(self.state_dims, self.action_dims, \n",
        "                                    self.learning_rate, 256, 256)\n",
        "        \n",
        "        self.target_critic1 = Critic(self.state_dims, self.action_dims, \n",
        "                                    self.learning_rate, 256, 256)\n",
        "        self.target_critic2 = Critic(self.state_dims, self.action_dims, \n",
        "                                    self.learning_rate, 256, 256)\n",
        "        \n",
        "        # TODO soft = hard update target and local\n",
        "        self.soft_update(1)\n",
        "\n",
        "        # initialize actor (pi)\n",
        "        self.actor = Actor(self.state_dims, self.action_dims, \n",
        "                           self.learning_rate, 256, 256)\n",
        "    \n",
        "        self.target_entropy = 0.98 * -np.log(1/action_dims)\n",
        "\n",
        "        # initalize alpha learning network\n",
        "        self.log_alpha = torch.tensor(np.log(self.alpha), requires_grad=True)\n",
        "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], \n",
        "                                                lr=self.learning_rate)\n",
        "    \n",
        "        \n",
        "    # Actor helper methods (all inputs are pytorch tensors)\n",
        "    def get_next_action(self, state, on_policy=False):\n",
        "        policy, _ = self.get_action_probs(state)\n",
        "        policy = policy.squeeze(0).detach().numpy()\n",
        "        if on_policy:\n",
        "            return np.argmax(policy)\n",
        "        else:\n",
        "            return np.random.choice(range(self.action_dims, p=policy))\n",
        "\n",
        "    def get_action_probs(self, state):\n",
        "        action_probs = self.actor.forward(state)\n",
        "        log_action_probs = torch.log(action_probs + 1e-8 if action_probs == 0.0 \n",
        "                            else torch.log(action_probs))    \n",
        "        \n",
        "        return action_probs, log_action_probs\n",
        "\n",
        "    def get_actor_loss(self, state):\n",
        "        action_probs, log_action_probs = self.get_action_probs(state)\n",
        "        \n",
        "        q_locals = get_quality(state)\n",
        "\n",
        "        actor_loss = (action_probs * \n",
        "                    (self.alpha * \n",
        "                     (log_action_probs - q_locals))).sum(dim=1).mean()\n",
        "        \n",
        "        return actor_loss, log_action_probs\n",
        "\n",
        "    # Alpha helper\n",
        "    def get_log_alpha_loss(self, log_action_probs):\n",
        "        return -(self.log_alpha * \n",
        "                (log_action_probs + self.target_entropy).detach()).mean()\n",
        "\n",
        "    # Critic helper methods (all inputs are pytorch tensors)\n",
        "    def get_quality(self, state):\n",
        "        return torch.min(self.local_critic1(state), self.local_critic2(state))\n",
        "\n",
        "    def get_target_quality(self, state):\n",
        "        return torch.min(self.target_critic1(state), self.target_critic2(state))\n",
        "\n",
        "    def get_critic_loss(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            action_probs, log_action_probs = self.get_action_probs(next_state)\n",
        "\n",
        "            q_next_target = self.get_target_quality(next_state)\n",
        "\n",
        "            soft_state_values = (action_probs * \n",
        "                                (q_next_target - \n",
        "                                (self.alpha * log_action_probs))).sum(dim=1)\n",
        "                \n",
        "            q_next = reward + ~done * self.gamma * soft_state_values\n",
        "\n",
        "        q_soft1 = self.local_critic1(state).gather(1, action.unsqueeze(-1)).squeezed(-1)\n",
        "        q_soft2 = self.local_critic1(state).gather(1, action.unsqueeze(-1)).squeezed(-1)\n",
        "\n",
        "        critic1_loss = (nn.MSELoss(reduction=\"none\")(q_soft1, q_next)).mean()\n",
        "        critic2_loss = (nn.MSELoss(reduction=\"none\")(q_soft2, q_next)).mean()\n",
        "\n",
        "        return critic1_loss, critic2_loss\n",
        "\n",
        "    def soft_update(self, tau):\n",
        "        for target_param, local_param in zip(target_critic1.parameters(), \n",
        "                                            local_critic1.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + \n",
        "                                    (1-tau) * target_param.data)\n",
        "        \n",
        "        for target_param, local_param in zip(target_critic2.parameters(), \n",
        "                                            local_critic2.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + \n",
        "                                    (1-tau) * target_param.data)\n",
        "\n",
        "    # train network on one state\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        self.local_critic1.optimizer.zero_grad()\n",
        "        self.local_critic2.optimizer.zero_grad()\n",
        "\n",
        "        self.actor.optimizer.zero_grad()\n",
        "\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            batch = self.replay_buffer.sample(self.batch_size)\n",
        "            batch = list(map(list, zip(*batch)))\n",
        "\n",
        "            state_tensor = torch.tensor(np.array(batch[0]))\n",
        "            action_tensor = torch.tensor(np.array(batch[1]))\n",
        "            reward_tensor = torch.tensor(np.array(batch[2])).float()\n",
        "            next_state_tensor = torch.tensor(np.array(batch[3]))\n",
        "            done_tensor = torch.tensor(np.array(batch[4]))\n",
        "\n",
        "            critic1_loss, critic2_loss = self.get_critic_loss(state_tensor,\n",
        "                                                        action_tensor,\n",
        "                                                        reward_tensor,\n",
        "                                                        next_state_tensor,\n",
        "                                                        done_tensor)\n",
        "            \n",
        "            critic1_loss.backward()\n",
        "            critic2_loss.backward()\n",
        "            self.local_critic1.optimizer.step()\n",
        "            self.local_critic2.optimizer.step()\n",
        "\n",
        "            actor_loss, log_action_probs = self.get_actor_loss(state_tensor)\n",
        "            actor_loss.backward()\n",
        "            self.actor.optimizer.step()\n",
        "\n",
        "            log_alpha_loss = self.get_log_alpha_loss(log_action_probs)\n",
        "            log_alpha_loss.backward()\n",
        "            self.log_alpha_optimizer.step()\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "\n",
        "            self.soft_update(self.tau)"
      ],
      "metadata": {
        "id": "8DFwfcfVcM2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "We create a custom environment fetching data from the Coinbase Exchange API. \n",
        "\n",
        "We define the state space as\n",
        "\n",
        "$$ \\mathcal{S}: \\mathbb{R}^5$$\n",
        "\n",
        "Where state $s_t$ = `[current holding, value(t), value(t-1), value(t-2), value(t-3)]`\n",
        "\n",
        "We define the action space as discrete\n",
        "$$ \\mathcal{A}: \\{-1,0,1\\} $$\n",
        "\n",
        "Where $a_t = -1 \\implies$ sell all, $a_t = 0 \\implies$ hold all, $a_t = 1 \\implies$ buy all.\n",
        "\n",
        "In the future, this action space can be modified from discrete to continuous [-1,1] for more precise calls.\n",
        "\n"
      ],
      "metadata": {
        "id": "jnnfPMqP07FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcYizbyB0-5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}